{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hello\n",
    "#### Let's have a look together at some basic NLP tasks with Leonardo Da Vinci\n",
    "\n",
    "#### You are listening 'Perfect Balance' - Jean-Philippe Rio-Py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Leonardo Da Vinci once said: When once you have tasted flight, you will forever walk the earth with your eyes turned skyward, for there you have been, and there you will always long to return.\n"
     ]
    }
   ],
   "source": [
    "# Sample text\n",
    "da_vinci_quote = 'When once you have tasted flight, you will forever walk the earth with your eyes turned skyward,\\\n",
    "for there you have been, and there you will always long to return.'\n",
    "print( 'Leonardo Da Vinci once said: {}'.format(leonardo_da_vinci_quote))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert to lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'when once you have tasted flight, you will forever walk the earth with your eyes turned skyward,for there you have been, and there you will always long to return.'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "da_vinci_quote.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Punctuation Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When once you have tasted flight  you will forever walk the earth with your eyes turned skyward for there you have been  and there you will always long to return \n"
     ]
    }
   ],
   "source": [
    "import re  # regular expression\n",
    "\n",
    "# We replace punctuations with a space \" \" because replacing with a space makes sure\n",
    "# that words don't get concatenated together.\n",
    "\n",
    "quote_no_punctuation = re.sub(r\"[^a-zA-Z0-9]\", \" \", da_vinci_quote)\n",
    "print(quote_no_punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['When', 'once', 'you', 'have', 'tasted', 'flight,', 'you', 'will', 'forever', 'walk', 'the', 'earth', 'with', 'your', 'eyes', 'turned', 'skyward,for', 'there', 'you', 'have', 'been,', 'and', 'there', 'you', 'will', 'always', 'long', 'to', 'return.']\n"
     ]
    }
   ],
   "source": [
    "# Split text into tokens (words)\n",
    "words = da_vinci_quote.split()\n",
    "print('Tokens: {}'.format(words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLTK: Natural Language ToolKit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install nltk # if you do not have it installed yet\n",
    "# !python -m nltk.downloader all\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Split text into a list of tokens (words) with word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['When', 'once', 'you', 'have', 'tasted', 'flight', ',', 'you', 'will', 'forever', 'walk', 'the', 'earth', 'with', 'your', 'eyes', 'turned', 'skyward', ',', 'for', 'there', 'you', 'have', 'been', ',', 'and', 'there', 'you', 'will', 'always', 'long', 'to', 'return', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "words = word_tokenize(da_vinci_quote)\n",
    "print('Tokens: {}'.format(words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split text into a list of sentences with 'sent_tokenize'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Da Vinci: Simplicity is the ultimate sophistication.Art is never finished, only abandoned. Learning never exhausts the mind.\n",
      "----------\n",
      "Sentences (Tokens): ['Simplicity is the ultimate sophistication.Art is never finished, only abandoned.', 'Learning never exhausts the mind.']\n"
     ]
    }
   ],
   "source": [
    "Da_Vinci_quotes = 'Simplicity is the ultimate sophistication.\\\n",
    "Art is never finished, only abandoned. Learning never exhausts the mind.'\n",
    "print(\"Da Vinci: {}\".format(Da_Vinci_quotes))\n",
    "\n",
    "print('----------')\n",
    "\n",
    "from nltk.tokenize import sent_tokenize\n",
    "sentences = sent_tokenize(Da_Vinci_quotes)\n",
    "print(\"Sentences (Tokens): {}\".format(sentences))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### list of stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be']\n"
     ]
    }
   ],
   "source": [
    "# List stop words\n",
    "from nltk.corpus import stopwords\n",
    "# In English\n",
    "print(stopwords.words(\"English\")[:50]) # list of the first 50 stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['de', 'la', 'que', 'el', 'en', 'y', 'a', 'los', 'del', 'se', 'las', 'por', 'un', 'para', 'con', 'no', 'una', 'su', 'al', 'lo', 'como', 'más', 'pero', 'sus', 'le', 'ya', 'o', 'este', 'sí', 'porque', 'esta', 'entre', 'cuando', 'muy', 'sin', 'sobre', 'también', 'me', 'hasta', 'hay', 'donde', 'quien', 'desde', 'todo', 'nos', 'durante', 'todos', 'uno', 'les', 'ni']\n"
     ]
    }
   ],
   "source": [
    "# In Spanish\n",
    "print(stopwords.words(\"Spanish\")[:50]) # list of the first 50 stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized text: ['simplicity', 'is', 'the', 'ultimate', 'sophistication', 'art', 'is', 'never', 'finished', 'only', 'abandoned', 'learning', 'never', 'exhausts', 'the', 'mind']\n"
     ]
    }
   ],
   "source": [
    "# First: Normalize text\n",
    "norm_leonardo = re.sub(r\"[^a-zA-Z0-9]\", \" \", Da_Vinci_quotes.lower())\n",
    "\n",
    "# Second: Tokenize it in a list of words\n",
    "words = norm_leonardo.split()\n",
    "print('Normalized text: {}'.format(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text without stop words: ['simplicity', 'ultimate', 'sophistication', 'art', 'never', 'finished', 'abandoned', 'learning', 'never', 'exhausts', 'mind']\n"
     ]
    }
   ],
   "source": [
    "# Remove stop words\n",
    "words1 = [w for w in words if w not in stopwords.words(\"english\")]\n",
    "print('Text without stop words: {}'.format(words1))\n",
    "# compare words with words1. Do you see the differences?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming and Lemmatisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In linguistic morphology and information retrieval:\n",
    "\n",
    "#### Stemming\n",
    "is the process of reducing inflected words to their word stem, base or root form—generally a written word form.\n",
    "\n",
    "adjustable --> adjust or changing --> chang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "words: ['simplicity', 'is', 'the', 'ultimate', 'sophistication', 'art', 'is', 'never', 'finished', 'only', 'abandoned', 'learning', 'never', 'exhausts', 'the', 'mind']\n",
      "------------------------\n",
      "stemmed: ['simplic', 'is', 'the', 'ultim', 'sophist', 'art', 'is', 'never', 'finish', 'onli', 'abandon', 'learn', 'never', 'exhaust', 'the', 'mind']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "# Reduce words to their stems\n",
    "words = norm_leonardo.split()\n",
    "stemmed = [PorterStemmer().stem(w) for w in words]\n",
    "print( 'words:', words)\n",
    "print('------------------------')\n",
    "print('stemmed:', stemmed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lemmatisation\n",
    "is the process of grouping together the inflected forms of a word so they can be analysed as a single item, identified by the word's lemma, or dictionary form\n",
    "\n",
    "paintings --> painting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text: ['The', 'Mona', 'Lisa', 'is', 'one', 'of', 'the', 'most', 'valuable', 'paintings', 'in', 'the', 'world.']\n",
      "------------------------\n",
      "lemmed: ['The', 'Mona', 'Lisa', 'is', 'one', 'of', 'the', 'most', 'valuable', 'painting', 'in', 'the', 'world.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "# Reduce words to their root form\n",
    "text = 'The Mona Lisa is one of the most valuable paintings in the world.'\n",
    "text = text.split(' ')\n",
    "lemmed = [WordNetLemmatizer().lemmatize(w) for w in text]\n",
    "print( 'text: {}'.format(text))\n",
    "print('------------------------')\n",
    "print('lemmed: {}'.format(lemmed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "paintings -> painting\n"
     ]
    }
   ],
   "source": [
    "# Did you notice?\n",
    "print('paintings -> {}'.format(WordNetLemmatizer().lemmatize('paintings')))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Follow Jabraghe:Learn with Classical Music on Facebook and Youtube.\n",
    "#### Thanks for Watching and Listening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
